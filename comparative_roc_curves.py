# -*- coding: utf-8 -*-
"""Comparative ROC Curves.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U201dYHTCnQH3-N1Gka5D93aDukfIoKt
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import pandas as pd

file_path = '/content/drive/MyDrive/MyImplementation/Functional_Similarity.xlsx'
Fun_Sim = pd.read_excel(file_path)
print(Fun_Sim)

import pandas as pd

# Assuming the provided similarity matrix is stored in Fun_Sim
file_path = '/content/drive/MyDrive/MyImplementation/Functional_Similarity.xlsx'
Fun_Sim = pd.read_excel(file_path, index_col=0)  # Assuming the first column contains row labels

# Reading disease IDs
file_path = '/content/drive/MyDrive/MyImplementation/Unique 47 diseases.xlsx'
disease_ids = pd.read_excel(file_path)

# Extracting disease pairs and their similarity values
num_diseases = Fun_Sim.shape[0]
pairs = []
for i in range(num_diseases):
    for j in range(i, num_diseases):  # Changed the starting index to include self-similarity
        disease_id1 = disease_ids.iloc[i, 0]  # Assuming the first column contains disease IDs
        disease_id2 = disease_ids.iloc[j, 0]
        pairs.append(((disease_id1, disease_id2), Fun_Sim.iloc[i, j]))

# Sorting pairs based on similarity values in descending order
sorted_pairs = sorted(pairs, key=lambda x: x[1], reverse=True)

# Create a DataFrame to store the sorted pairs
df_sorted_pairs = pd.DataFrame(sorted_pairs, columns=['Disease Pair', 'Similarity'])

# Export the DataFrame to Excel
output_file_path = '/content/drive/MyDrive/MyImplementation/Fun_Sim_Sorted_Similarity.xlsx'
df_sorted_pairs.to_excel(output_file_path, index=False)

print("Sorted pairs exported to Excel successfully.")

import pandas as pd

file_path = '/content/drive/MyDrive/MyImplementation/normalized_matrix_cosine.xlsx'
Network_Cosine_Sim = pd.read_excel(file_path)
print(Network_Cosine_Sim)

import pandas as pd

# Assuming the provided similarity matrix is stored in Fun_Sim
file_path = '/content/drive/MyDrive/MyImplementation/normalized_matrix_cosine.xlsx'
Network_Cosine_Sim = pd.read_excel(file_path)

# Reading disease IDs
file_path = '/content/drive/MyDrive/MyImplementation/Unique 47 diseases.xlsx'
disease_ids = pd.read_excel(file_path)

# Convert similarity values to numeric data type
Network_Cosine_Sim = Network_Cosine_Sim.apply(pd.to_numeric, errors='coerce')

# Extracting disease pairs and their similarity values
num_diseases = Network_Cosine_Sim.shape[0]
pairs = []
for i in range(num_diseases):
    for j in range(i, num_diseases):  # Changed the starting index to include self-similarity
        disease_id1 = disease_ids.iloc[i, 0]  # Assuming the first column contains disease IDs
        disease_id2 = disease_ids.iloc[j, 0]
        pairs.append(((disease_id1, disease_id2), Network_Cosine_Sim.iloc[i, j]))

# Sorting pairs based on similarity values in descending order
sorted_pairs = sorted(pairs, key=lambda x: x[1], reverse=True)

# Create a DataFrame to store the sorted pairs
df_sorted_pairs = pd.DataFrame(sorted_pairs, columns=['Disease Pair', 'Similarity'])

# Export the DataFrame to Excel
output_file_path = '/content/drive/MyDrive/MyImplementation/Network_Cosine_Sim_Sorted_Similarity.xlsx'
df_sorted_pairs.to_excel(output_file_path, index=False)

print("Sorted pairs exported to Excel successfully.")

install.packages("DOSE")

library(DOSE)

R.version

if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("DOSE")

library(DOSE)

library(readxl)

df <- read_excel("/content/Unique 47 diseases.xlsx")

s <- doSim(df, df, measure="Wang")

library(readr)

write.csv(as.data.frame(s), "47_similarity_matrix_WANG.csv", row.names = TRUE)

s <- doSim(df, df, measure="Resnik")

write.csv(as.data.frame(s), "47_similarity_matrix_Resnik.csv", row.names = TRUE)

s <- doSim(df, df, measure="Rel")

write.csv(as.data.frame(s), "47_similarity_matrix_Rel.csv", row.names = TRUE)

s <- doSim(df, df, measure="Jiang")

write.csv(as.data.frame(s), "47_similarity_matrix_Jiang.csv", row.names = TRUE)

s <- doSim(df, df, measure="Lin")

write.csv(as.data.frame(s), "47_similarity_matrix_Lin.csv", row.names = TRUE)

"""**Semantic Similarity**"""

import pandas as pd
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt


file_path8 = '/content/47_similarity_matrix_Jiang.csv'
predicted_Jiang = pd.read_csv(file_path8).set_index('DOID')


file_path9 = '/content/true positives.xlsx'
actual_df = pd.read_excel(file_path9).set_index('DOID')


# Combine the actual and predicted values for each pair of diseases
actual_values = actual_df.values.flatten()
predicted_values = predicted_Jiang.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Jiang')
plt.legend(loc='lower right')
plt.show()

import pandas as pd
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt


file_path8 = '/content/47_similarity_matrix_Lin.csv'
predicted_Lin = pd.read_csv(file_path8).set_index('DOID')

file_path9 = '/content/true positives.xlsx'
actual_df = pd.read_excel(file_path9).set_index('DOID')

# Combine the actual and predicted values for each pair of diseases
actual_values = actual_df.values.flatten()
predicted_values = predicted_Lin.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Lin')
plt.legend(loc='lower right')
plt.show()

import pandas as pd
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt


file_path8 = '/content/47_similarity_matrix_Rel.csv'
predicted_Rel = pd.read_csv(file_path8).set_index('DOID')

file_path9 = '/content/true positives.xlsx'
actual_df = pd.read_excel(file_path9).set_index('DOID')

# Combine the actual and predicted values for each pair of diseases
actual_values = actual_df.values.flatten()
predicted_values = predicted_Rel.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Rel')
plt.legend(loc='lower right')
plt.show()

import pandas as pd
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt


file_path8 = '/content/47_similarity_matrix_Resnik.csv'
predicted_Resnik = pd.read_csv(file_path8).set_index('DOID')

file_path9 = '/content/true positives.xlsx'
actual_df = pd.read_excel(file_path9).set_index('DOID')

# Combine the actual and predicted values for each pair of diseases
actual_values = actual_df.values.flatten()
predicted_values = predicted_Resnik.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Resnik')
plt.legend(loc='lower right')
plt.show()

import pandas as pd
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt


file_path8 = '/content/47_similarity_matrix_WANG (1).csv'
predicted_Wang = pd.read_csv(file_path8).set_index('DOID')

file_path9 = '/content/true positives.xlsx'
actual_df = pd.read_excel(file_path9).set_index('DOID')

# Combine the actual and predicted values for each pair of diseases
actual_values = actual_df.values.flatten()
predicted_values = predicted_Wang.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Wang')
plt.legend(loc='lower right')
plt.show()

import pandas as pd
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt


file_path8 = '/content/semfunsim_matrix_final.xlsx'
predicted_Cheng = pd.read_excel(file_path8).set_index('DOID')

file_path9 = '/content/true positives.xlsx'
actual_df = pd.read_excel(file_path9).set_index('DOID')

# Combine the actual and predicted values for each pair of diseases
actual_values = actual_df.values.flatten()
predicted_values = predicted_Cheng.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Cheng')
plt.legend(loc='lower right')
plt.show()

import pandas as pd
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Assuming you have actual_df and predicted_df dataframes for different methods

# Define methods and corresponding dataframes
methods = ['Cheng', 'Jiang', 'Lin', 'Rel', 'Resnik', 'WANG', ]  # Add more methods as needed
dataframes = [predicted_Cheng, predicted_Jiang, predicted_Lin, predicted_Rel, predicted_Resnik, predicted_Wang,  ]  # Add corresponding dataframes

plt.figure(figsize=(10, 6))

for method, predicted_df in zip(methods, dataframes):
    # Combine the actual and predicted values for each pair of diseases
    actual_values = actual_df.values.flatten()
    predicted_values = predicted_df.values.flatten()

    # Compute the ROC curve
    fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

    # Compute the Area Under the Curve (AUC)
    roc_auc = auc(fpr, tpr)

    # Plot the ROC curve
    plt.plot(fpr, tpr, lw=2, label=f'{method} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Different Methods')
plt.legend(loc='lower right')
plt.show()

"""**Network Similarity**"""

import pandas as pd
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt


file_path8 = '/content/normalized_matrix_jaccard.xlsx'
predicted_jaccard = pd.read_excel(file_path8).set_index('DOID')

file_path9 = '/content/true positives.xlsx'
actual_df = pd.read_excel(file_path9).set_index('DOID')

# Combine the actual and predicted values for each pair of diseases
actual_values = actual_df.values.flatten()
predicted_values = predicted_jaccard.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Jaccard')
plt.legend(loc='lower right')
plt.show()

import pandas as pd
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt


file_path8 = '/content/normalized_matrix_overlap.xlsx'
predicted_overlap = pd.read_excel(file_path8).set_index('DOID')

file_path9 = '/content/true positives.xlsx'
actual_df = pd.read_excel(file_path9).set_index('DOID')

# Combine the actual and predicted values for each pair of diseases
actual_values = actual_df.values.flatten()
predicted_values = predicted_overlap.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Overlap')
plt.legend(loc='lower right')
plt.show()

import pandas as pd
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt


file_path8 = '/content/normalized_matrix_sorensen.xlsx'
predicted_sorensen = pd.read_excel(file_path8).set_index('DOID')

file_path9 = '/content/true positives.xlsx'
actual_df = pd.read_excel(file_path9).set_index('DOID')

# Combine the actual and predicted values for each pair of diseases
actual_values = actual_df.values.flatten()
predicted_values = predicted_sorensen.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Sorensen')
plt.legend(loc='lower right')
plt.show()

import pandas as pd
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt


file_path8 = '/content/normalized_matrix_pearson.xlsx'
predicted_pearson = pd.read_excel(file_path8).set_index('DOID')

file_path9 = '/content/true positives.xlsx'
actual_df = pd.read_excel(file_path9).set_index('DOID')

# Combine the actual and predicted values for each pair of diseases
actual_values = actual_df.values.flatten()
predicted_values = predicted_pearson.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Yanjun')
plt.legend(loc='lower right')
plt.show()

import pandas as pd
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt


file_path8 = '/content/normalized_matrix_cosine.xlsx'
predicted_cosine = pd.read_excel(file_path8).set_index('DOID')

file_path9 = '/content/true positives.xlsx'
actual_df = pd.read_excel(file_path9).set_index('DOID')

# Combine the actual and predicted values for each pair of diseases
actual_values = actual_df.values.flatten()
predicted_values = predicted_cosine.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Cosine')
plt.legend(loc='lower right')
plt.show()

import pandas as pd
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Assuming you have actual_df and predicted_df dataframes for different methods

# Define methods and corresponding dataframes
methods = ['S_Cosine', 'Yanjun', 'Jaccard', 'Overlap', 'Sorensen']  # Add more methods as needed
dataframes = [predicted_cosine, predicted_pearson, predicted_jaccard, predicted_overlap, predicted_sorensen,  ]  # Add corresponding dataframes

plt.figure(figsize=(10, 6))

for method, predicted_df in zip(methods, dataframes):
    # Combine the actual and predicted values for each pair of diseases
    actual_values = actual_df.values.flatten()
    predicted_values = predicted_df.values.flatten()

    # Compute the ROC curve
    fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

    # Compute the Area Under the Curve (AUC)
    roc_auc = auc(fpr, tpr)

    # Plot the ROC curve
    plt.plot(fpr, tpr, lw=2, label=f'{method} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Different Methods')
plt.legend(loc='lower right')
plt.show()

import pandas as pd
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt


file_path8 = '/content/Functional_Similarity.xlsx'
predicted_fun = pd.read_excel(file_path8).set_index('DOID')

file_path9 = '/content/true positives.xlsx'
actual_df = pd.read_excel(file_path9).set_index('DOID')

# Combine the actual and predicted values for each pair of diseases
actual_values = actual_df.values.flatten()
predicted_values = predicted_fun.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Cheng (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Functional')
plt.legend(loc='lower right')
plt.show()

import pandas as pd
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Assuming you have actual_df and predicted_df dataframes for different methods

# Define methods and corresponding dataframes
methods = ['Cheng_Functional','S_Cosine_Network', 'Cheng_Semantic']  # Add more methods as needed
dataframes = [predicted_fun, predicted_cosine, predicted_Cheng  ]  # Add corresponding dataframes

plt.figure(figsize=(10, 6))

for method, predicted_df in zip(methods, dataframes):
    # Combine the actual and predicted values for each pair of diseases
    actual_values = actual_df.values.flatten()
    predicted_values = predicted_df.values.flatten()

    # Compute the ROC curve
    fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

    # Compute the Area Under the Curve (AUC)
    roc_auc = auc(fpr, tpr)

    # Plot the ROC curve
    plt.plot(fpr, tpr, lw=2, label=f'{method} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Different Methods')
plt.legend(loc='lower right')
plt.show()

import pandas as pd
import numpy as np

file_path8 = '/content/semfunsim_matrix_final.xlsx'
predicted_Cheng = pd.read_excel(file_path8).set_index('DOID')
print('Semantic')
print(predicted_Cheng)

file_path8 = '/content/Functional_Similarity.xlsx'
predicted_fun = pd.read_excel(file_path8).set_index('DOID')
print('Functional')
print(predicted_fun)

file_path8 = '/content/normalized_matrix_cosine.xlsx'
predicted_cosine = pd.read_excel(file_path8).set_index('DOID')
print('Network')
print(predicted_cosine)

# Perform element-wise multiplication excluding the index column  S*F+N
result = (predicted_Cheng.values * predicted_fun.values * predicted_cosine)

# Create a new DataFrame with the same index and columns
result_df = pd.DataFrame(result, index=predicted_Cheng.index, columns=predicted_Cheng.columns)

# Print the result
print(result_df)

import pandas as pd

# Assuming you have three DataFrames containing similarity values
# For example:
# predicted_Cheng, predicted_fun, and predicted_cosine

file_path8 = '/content/semfunsim_matrix_final_updated.xlsx'
predicted_Cheng = pd.read_excel(file_path8).set_index('DOID')
print('Semantic')
print(predicted_Cheng)

file_path8 = '/content/Functional_Similarity.xlsx'
predicted_fun = pd.read_excel(file_path8).set_index('DOID')
print('Functional')
print(predicted_fun)

file_path8 = '/content/normalized_matrix_cosine.xlsx'
predicted_cosine = pd.read_excel(file_path8).set_index('DOID')
print('Network')
print(predicted_cosine)


# Perform element-wise multiplication of the first two DataFrames
result_df = (0.2 * predicted_Cheng) * (0.6 * predicted_fun) + (0.2 * predicted_cosine )

# Print the resultant DataFrame
print(result_df)

import pandas as pd
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt


file_path9 = '/content/true positives.xlsx'
actual_df = pd.read_excel(file_path9).set_index('DOID')

# Combine the actual and predicted values for each pair of diseases
actual_values = actual_df.values.flatten()
predicted_values = result_df.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Disease Disease Similarity')
plt.legend(loc='lower right')
plt.show()

import pandas as pd
file_path8 = '/content/semfunsim_matrix_final_updated.xlsx'
df = pd.read_excel(file_path8).set_index('DOID')
print('Semantic')
print(df)

# Stack the DataFrame to create a Series with pairs and similarity values
stacked = df.stack().reset_index()

# Rename columns for clarity
stacked.columns = ['Disease 1', 'Disease 2', 'Similarity']

# Filter out rows where Disease 1 is the same as Disease 2 (self-similarity)
stacked = stacked[stacked['Disease 1'] != stacked['Disease 2']]

# Sort by 'Similarity' in descending order
sorted_pairs = stacked.sort_values(by='Similarity', ascending=False)

# Reset index for the sorted DataFrame (optional)
sorted_pairs.reset_index(drop=True, inplace=True)

# Display the sorted DataFrame
print(sorted_pairs)

import pandas as pd

# Assuming 'stacked' is your DataFrame after stacking and filtering
# For illustration, skipping the initial stacking and filtering steps

# Sort by 'Similarity' in descending order
sorted_pairs = stacked.sort_values(by='Similarity', ascending=False)

# Reset index for the sorted DataFrame (optional)
sorted_pairs.reset_index(drop=True, inplace=True)

# Display the sorted DataFrame
print(sorted_pairs)

file_path = '/content/sorted_disease_pairs.xlsx'

sorted_pairs.to_excel(file_path, index=False)

print(f"DataFrame successfully exported to {file_path}")

pip install matplotlib-venn pandas

import pandas as pd
from matplotlib import pyplot as plt
from matplotlib_venn import venn3

# Example DataFrames (replace these with your actual DataFrames)
data1 = {
    'Disease 1': ['A', 'B', 'C'],
    'Disease 2': ['D', 'E', 'F'],
    'Similarity': [0.5, 0.3, 0.4]
}
df1 = pd.DataFrame(data1)

data2 = {
    'Disease 1': ['A', 'C', 'E'],
    'Disease 2': ['B', 'D', 'F'],
    'Similarity': [0.6, 0.2, 0.7]
}
df2 = pd.DataFrame(data2)

data3 = {
    'Disease 1': ['A', 'B', 'D'],
    'Disease 2': ['C', 'E', 'F'],
    'Similarity': [0.8, 0.1, 0.9]
}
df3 = pd.DataFrame(data3)

# Convert each DataFrame to a set of tuples for Venn diagram plotting
set1 = set(zip(df1['Disease 1'], df1['Disease 2']))
set2 = set(zip(df2['Disease 1'], df2['Disease 2']))
set3 = set(zip(df3['Disease 1'], df3['Disease 2']))

# Calculate intersections
intersection = {
    '100': set1 - set2 - set3,
    '010': set2 - set1 - set3,
    '001': set3 - set1 - set2,
    '110': set1 & set2 - set3,
    '101': set1 & set3 - set2,
    '011': set2 & set3 - set1,
    '111': set1 & set2 & set3
}

# Plot Venn diagram with circles for each DataFrame
plt.figure(figsize=(8, 8))
venn3(subsets=(len(intersection['100']), len(intersection['010']), len(intersection['110']),
              len(intersection['001']), len(intersection['101']), len(intersection['011']),
              len(intersection['111'])),
      set_labels=('df1', 'df2', 'df3'))
plt.title('Venn Diagram of Disease Pairs')
plt.show()

import pandas as pd

# Example DataFrames (replace with your actual DataFrames)
data1 = {
    'Disease 1': ['A', 'B', 'C'],
    'Disease 2': ['D', 'E', 'D'],
    'Similarity': [0.5, 0.3, 0.4]
}
df1 = pd.DataFrame(data1)

data2 = {
    'Disease 1': ['A', 'C', 'B'],
    'Disease 2': ['D', 'D', 'E'],
    'Similarity': [0.6, 0.2, 0.7]
}
df2 = pd.DataFrame(data2)

# Convert each DataFrame to sets of tuples (Disease 1, Disease 2)
set_df1 = set(zip(df1['Disease 1'], df1['Disease 2']))
set_df2 = set(zip(df2['Disease 1'], df2['Disease 2']))

# Find common disease pairs
common_pairs = set_df1 & set_df2

# Count the number of common pairs
count_common_pairs = len(common_pairs)

print(f"Number of common disease pairs between df1 and df2: {count_common_pairs}")

import pandas as pd

file_path8 = '/content/Fun_Sim Sorted Values.xlsx'
Fun = pd.read_excel(file_path8)
#print(Fun)

file_path9 = '/content/Sem_Sim Sorted Values.xlsx'
Sem = pd.read_excel(file_path9)
#print(Sem)

file_path10 = '/content//Net_Sim Sorted Values.xlsx'
Net = pd.read_excel(file_path10)
#print(Net)

# Convert each DataFrame to sets of tuples (Disease 1, Disease 2)
set_Fun = set(zip(Fun['DOID 1'], Fun['DOID 2']))
set_Sem = set(zip(Sem['Disease 1'], Sem['Disease 2']))

# Find common disease pairs
common_pairs = set_Fun & set_Sem

# Count the number of common pairs
count_common_pairs_FunSem = len(common_pairs)

print(f"Number of common disease pairs between Fun and Sem: {count_common_pairs}")

# Convert each DataFrame to sets of tuples (Disease 1, Disease 2)
set_Fun = set(zip(Fun['DOID 1'], Fun['DOID 2']))
set_Net = set(zip(Net['DOID 1'], Net['DOID 2']))

# Find common disease pairs
common_pairs = set_Fun & set_Net

# Count the number of common pairs
count_common_pairs_FunNet = len(common_pairs)

print(f"Number of common disease pairs between Fun and Net: {count_common_pairs}")

set_Sem = set(zip(Sem['Disease 1'], Sem['Disease 2']))
set_Net = set(zip(Net['DOID 1'], Net['DOID 2']))

# Find common disease pairs
common_pairs = set_Net & set_Sem

# Count the number of common pairs
count_common_pairs_NetSem = len(common_pairs)

print(f"Number of common disease pairs between Net and Sem: {count_common_pairs}")

set_Fun = set(zip(Fun['DOID 1'], Fun['DOID 2']))
set_Net = set(zip(Net['DOID 1'], Net['DOID 2']))
set_Sem = set(zip(Sem['Disease 1'], Sem['Disease 2']))

# Find common disease pairs
common_pairs = set_Fun & set_Net & set_Sem

# Count the number of common pairs
count_common_pairs_FunNetSem = len(common_pairs)

print(f"Number of common disease pairs between Fun, Sem and Net: {count_common_pairs}")

import pandas as pd

file_path8 = '/content/Fun_Sim Sorted Values.xlsx'
data1 = pd.read_excel(file_path8)
df1 = pd.DataFrame(data1)


file_path9 = '/content/Sem_Sim Sorted Values.xlsx'
data2 = pd.read_excel(file_path9)
df2 = pd.DataFrame(data2)

# Merge on Disease 1 to Disease 2 direction
merge_df1 = pd.merge(df1, df2, on=['Disease 1', 'Disease 2'], how='inner')

# Merge on Disease 2 to Disease 1 direction
merge_df2 = pd.merge(df1, df2, left_on=['Disease 1', 'Disease 2'], right_on=['Disease 2', 'Disease 1'], how='inner')

# Concatenate both merge results and drop duplicates
common_pairs = pd.concat([merge_df1[['Disease 1', 'Disease 2']], merge_df2[['Disease 1_x', 'Disease 2_x']].rename(columns={'Disease 1_x': 'Disease 1', 'Disease 2_x': 'Disease 2'})])
common_pairs.drop_duplicates(inplace=True)

# Display common pairs
print("Common Disease Pairs:")
print(common_pairs)

import pandas as pd

file_path8 = '/content/Net_Sim Sorted Values.xlsx'
data1 = pd.read_excel(file_path8)
df1 = pd.DataFrame(data1)


file_path9 = '/content/Sem_Sim Sorted Values.xlsx'
data2 = pd.read_excel(file_path9)
df2 = pd.DataFrame(data2)

# Merge on Disease 1 to Disease 2 direction
merge_df1 = pd.merge(df1, df2, on=['Disease 1', 'Disease 2'], how='inner')

# Merge on Disease 2 to Disease 1 direction
merge_df2 = pd.merge(df1, df2, left_on=['Disease 1', 'Disease 2'], right_on=['Disease 2', 'Disease 1'], how='inner')

# Concatenate both merge results and drop duplicates
common_pairs = pd.concat([merge_df1[['Disease 1', 'Disease 2']], merge_df2[['Disease 1_x', 'Disease 2_x']].rename(columns={'Disease 1_x': 'Disease 1', 'Disease 2_x': 'Disease 2'})])
common_pairs.drop_duplicates(inplace=True)

# Display common pairs
print("Common Disease Pairs:")
print(common_pairs)

import pandas as pd

file_path8 = '/content/Net_Sim Sorted Values.xlsx'
data1 = pd.read_excel(file_path8)
df1 = pd.DataFrame(data1)


file_path9 = '/content/Fun_Sim Sorted Values.xlsx'
data2 = pd.read_excel(file_path9)
df2 = pd.DataFrame(data2)

# Merge on Disease 1 to Disease 2 direction
merge_df1 = pd.merge(df1, df2, on=['Disease 1', 'Disease 2'], how='inner')

# Merge on Disease 2 to Disease 1 direction
merge_df2 = pd.merge(df1, df2, left_on=['Disease 1', 'Disease 2'], right_on=['Disease 2', 'Disease 1'], how='inner')

# Concatenate both merge results and drop duplicates
common_pairs = pd.concat([merge_df1[['Disease 1', 'Disease 2']], merge_df2[['Disease 1_x', 'Disease 2_x']].rename(columns={'Disease 1_x': 'Disease 1', 'Disease 2_x': 'Disease 2'})])
common_pairs.drop_duplicates(inplace=True)

# Display common pairs
print("Common Disease Pairs:")
print(common_pairs)

import pandas as pd
file_path8 = '/content/integrated_similarity_values.xlsx'
df = pd.read_excel(file_path8).set_index('DOID')
print('Integrated')
#print(df)

# Stack the DataFrame to create a Series with pairs and similarity values
stacked = df.stack().reset_index()

# Rename columns for clarity
stacked.columns = ['Disease 1', 'Disease 2', 'Similarity']

# Filter out rows where Disease 1 is the same as Disease 2 (self-similarity)
stacked = stacked[stacked['Disease 1'] != stacked['Disease 2']]

# Sort by 'Similarity' in descending order
sorted_pairs_integrated = stacked.sort_values(by='Similarity', ascending=False)

# Reset index for the sorted DataFrame (optional)
sorted_pairs_integrated.reset_index(drop=True, inplace=True)

# Display the sorted DataFrame
print(sorted_pairs_integrated)

# Export result_df_1111 to Excel
output_file_path = '/content/integrated_similarity_values_sorted.xlsx'
sorted_pairs_integrated.to_excel(output_file_path)

print(f"DataFrame exported to {output_file_path}")

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

import pandas as pd
# Load Excel file from Google Drive
file_path = '/content/drive/MyDrive/Fun Sim/DOID and GENE ID.xlsx'
SGID = pd.read_excel(file_path)
print(SGID)

import pandas as pd
from itertools import combinations
from collections import Counter


file_path = '/content/drive/MyDrive/Fun Sim/DOID and GENE ID.xlsx'
df1 = pd.read_excel(file_path)
df = pd.DataFrame(df1)

# Group by DOID and collect Gene IDs
grouped = df.groupby('DOID')['Gene ID'].apply(list).reset_index()

# Function to find number of common elements between two lists
def count_common_elements(list1, list2):
    return len(set(list1) & set(list2))

# Find all pairs of DOID and calculate number of common Gene IDs
doid_pairs = combinations(grouped['DOID'], 2)
common_gene_counts = []
for d1, d2 in doid_pairs:
    genes1 = grouped[grouped['DOID'] == d1]['Gene ID'].iloc[0]
    genes2 = grouped[grouped['DOID'] == d2]['Gene ID'].iloc[0]
    common_count = count_common_elements(genes1, genes2)
    common_gene_counts.append((d1, d2, common_count))

# Convert to DataFrame and sort by common_gene_count in descending order
common_gene_df = pd.DataFrame(common_gene_counts, columns=['DOID1', 'DOID2', 'Common Gene Count'])
sorted_common_gene_df = common_gene_df.sort_values(by='Common Gene Count', ascending=False)

# Display the sorted DataFrame
print("Disease pairs sorted by shared common gene IDs:")
print(sorted_common_gene_df)

file_path = '/content/drive/MyDrive/Fun Sim/shared_genes _disease_pairs.xlsx'

sorted_common_gene_df.to_excel(file_path, index=False)

print(f"DataFrame successfully exported to {file_path}")

import pandas as pd
from itertools import combinations


file_path = '/content/drive/MyDrive/Fun Sim/DOID and GENE ID.xlsx'
df1 = pd.read_excel(file_path)
df = pd.DataFrame(df1)

# Group by DOID and collect Gene IDs into sets
grouped = df.groupby('DOID')['Gene ID'].apply(set).reset_index()

# Function to find number of common elements between two sets
def count_common_elements(set1, set2):
    return len(set1 & set2)

# Find all pairs of DOID and calculate number of common Gene IDs
doid_pairs = combinations(grouped['DOID'], 2)
common_gene_counts = []
for d1, d2 in doid_pairs:
    genes1 = grouped[grouped['DOID'] == d1]['Gene ID'].iloc[0]
    genes2 = grouped[grouped['DOID'] == d2]['Gene ID'].iloc[0]
    common_count = count_common_elements(genes1, genes2)
    common_gene_counts.append((d1, d2, common_count))

# Convert to DataFrame and sort by common_gene_count in descending order
common_gene_df = pd.DataFrame(common_gene_counts, columns=['DOID1', 'DOID2', 'Common Gene Count'])
sorted_common_gene_df = common_gene_df.sort_values(by='Common Gene Count', ascending=False)

# Display the sorted DataFrame
print("Disease pairs sorted by shared common gene IDs:")
print(sorted_common_gene_df)

import pandas as pd

file_path = '/content/drive/MyDrive/Fun Sim/Top Diseases based on shared genes.xlsx'
shared_genes = pd.read_excel(file_path)
print(shared_genes)

import pandas as pd
# Load Excel file from Google Drive
file_path = '/content/drive/MyDrive/Fun Sim/DOID and GENE ID.xlsx'
SGID = pd.read_excel(file_path)
print(SGID)

import pandas as pd


sgid_df = pd.DataFrame(SGID)


shared_genes_df = pd.DataFrame(shared_genes)

# Merge shared_genes_df with sgid_df on 'DOID'
result_df = pd.merge(shared_genes_df, sgid_df, on='DOID', how='left')

# Display the result
print(result_df)

file_path = '/content/drive/MyDrive/Fun Sim/shared_genes_diseases_counts.xlsx'

sorted_common_gene_df.to_excel(file_path, index=False)

print(f"DataFrame successfully exported to {file_path}")

import pandas as pd
# Load Excel file from Google Drive
file_path = '/content/Unique shared genes among top 10.xlsx'
genes = pd.read_excel(file_path)
print(genes)

import pandas as pd

file_path = '/content/drive/MyDrive/Fun Sim/Functional_Score11.xlsx'
F11 = pd.read_excel(file_path)

file_path = '/content/drive/MyDrive/Fun Sim/Functional_Score22.xlsx'
F22 = pd.read_excel(file_path)

file_path = '/content/drive/MyDrive/Fun Sim/Functional_Score33.xlsx'
F33 = pd.read_excel(file_path)

combined_df = pd.concat([F11, F22, F33], ignore_index=True)
print(combined_df)

import pandas as pd


df = pd.DataFrame(combined_df)

# Drop the 'score' column
df_without_score = df.drop(columns=['score'])

# Display the DataFrame without the 'score' column
print(df_without_score)

pip install networkx

import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt


genes_df = pd.DataFrame(genes)


# Extract unique nodes (genes) from genes_df
nodes = list(genes_df['Gene ID'])

# Create a graph object
G = nx.Graph()

# Add nodes to the graph
G.add_nodes_from(nodes)

# Add edges to the graph from df_without_score
edges = [(row['node1'], row['node2']) for idx, row in df_without_score.iterrows()]
G.add_edges_from(edges)

# Draw the graph
#plt.figure(figsize=(10, 8))
##pos = nx.spring_layout(G)  # Position nodes using a spring layout
#nx.draw(G, pos, with_labels=True, node_size=500, node_color='skyblue', font_size=10, font_color='black', edge_color='gray')
#plt.title('Genes and Connections')
#plt.show()

# Calculate the degree of each node
degree = dict(G.degree())

# Print the degree of each node
for node in nodes:
    print(f"Node {node}: Degree {degree[node]}")

# Calculate the degree of each node
degree = dict(G.degree())

# Sort nodes based on their degree in descending order
sorted_nodes = sorted(degree, key=degree.get, reverse=True)

# Print nodes and their degrees in descending order
for node in sorted_nodes:
    print(f"Node {node}: Degree {degree[node]}")

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

import pandas as pd
import io

# Assuming similarity_matrix_semfunsim is already loaded

# Min-Max normalization function
def min_max_normalize(matrix):
    matrix_array = matrix.values.astype(float)
    min_val = matrix_array.min()
    print(min_val)
    max_val = matrix_array.max()
    print(max_val)
    normalized_matrix = (matrix_array - min_val) / (max_val - min_val)
    return normalized_matrix

file_path3 = '/content/drive/MyDrive/Integration/semfunsim_matrix_final - Copy.xlsx'
semfunsim_df = pd.read_excel(file_path3).set_index('DOID')


file_path1 = '/content/drive/MyDrive/Fun Sim/Functional_Similarity.xlsx'
Functional_df= pd.read_excel(file_path1).set_index('DOID')


# Perform element-wise multiplication excluding the index column
result1 = semfunsim_df.values * Functional_df.values

# Create a new DataFrame with the same index and columns
SEMFUNSIM = pd.DataFrame(result1, index=semfunsim_df.index, columns=semfunsim_df.columns)

# Normalize the similarity matrix
normalized_similarity_matrix1 = min_max_normalize(SEMFUNSIM)

# Create a DataFrame for the normalized matrix
SEMFUNSIM_df = pd.DataFrame(normalized_similarity_matrix1, columns=semfunsim_df.columns, index=semfunsim_df.index)

import numpy as np


file_path8 = '/content/47_similarity_matrix_Resnik.csv'
Resnik = pd.read_csv(file_path8).set_index('DOID')

file_path1 = '/content/drive/MyDrive/Fun Sim/Functional_Similarity.xlsx'
Functional_df= pd.read_excel(file_path1).set_index('DOID')

file_path5 = '/content/drive/MyDrive/Integration/PCM.xlsx'
PCM_df = pd.read_excel(file_path5).set_index('DOID')

# Perform element-wise multiplication excluding the index column
result2 = np.sqrt(Resnik.values * Functional_df.values + PCM_df.values)

# Create a new DataFrame with the same index and columns
autoimmune = pd.DataFrame(result2, index=Functional_df.index, columns=Functional_df.columns)

# Normalize the similarity matrix
normalized_similarity_matrix11 = min_max_normalize(autoimmune)

# Create a DataFrame for the normalized matrix
autoimmune_df = pd.DataFrame(normalized_similarity_matrix11, columns=Functional_df.columns, index=Functional_df.index)

autoimmune_df.to_csv("results_autoimmune.csv", index=False)
print("Exported to results.csv")

import numpy as np


file_path8 = '/content/47_similarity_matrix_Resnik.csv'
Resnik = pd.read_csv(file_path8).set_index('DOID')

file_path1 = '/content/drive/MyDrive/Fun Sim/Functional_Similarity.xlsx'
Functional_df= pd.read_excel(file_path1).set_index('DOID')

file_path5 = '/content/drive/MyDrive/Integration/PCM.xlsx'
PCM_df = pd.read_excel(file_path5).set_index('DOID')

# Perform element-wise multiplication excluding the index column
result2 = np.sqrt(Resnik.values * Functional_df.values + PCM_df.values)

# Create a new DataFrame with the same index and columns
autoimmune = pd.DataFrame(result2, index=Functional_df.index, columns=Functional_df.columns)

import numpy as np
import pandas as pd

file_path3 = '/content/drive/MyDrive/Integration/semfunsim_matrix_final - Copy.xlsx'
semfunsim_df = pd.read_excel(file_path3).set_index('DOID')

file_path1 = '/content/drive/MyDrive/Fun Sim/Functional_Similarity.xlsx'
Functional_df= pd.read_excel(file_path1).set_index('DOID')

file_path5 = '/content/drive/MyDrive/Integration/CSM.xlsx'
CSM_df = pd.read_excel(file_path5).set_index('DOID')

# Perform element-wise multiplication excluding the index column
result3 = (semfunsim_df.values * Functional_df.values + CSM_df.values)

# Create a new DataFrame with the same index and columns
ours = pd.DataFrame(result3, index=semfunsim_df.index, columns=semfunsim_df.columns)

ours.to_csv("results_redicted.csv", index=False)
print("Exported to results.csv")

import pandas as pd
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Assuming you have actual_df and predicted_df dataframes
file_path2 = '/content/drive/MyDrive/Integration/true positives.xlsx'
actual_df = pd.read_excel(file_path2).set_index('DOID')

# Combine the actual and predicted values for each pair of diseases
actual_values = actual_df.values.flatten()
predicted_values = ours.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)


# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Functional Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

# Normalize the similarity matrix
normalized_similarity_matrix111 = min_max_normalize(ours)

# Create a DataFrame for the normalized matrix
ours_df = pd.DataFrame(normalized_similarity_matrix111, columns=Functional_df.columns, index=Functional_df.index)

ours_df.to_csv("results_redicted_normalized.csv", index=False)
print("Exported to results.csv")

import numpy as np
import pandas as pd

file_path1 = '/content/drive/MyDrive/Fun Sim/Functional_Similarity.xlsx'
FunSim= pd.read_excel(file_path1).set_index('DOID')

# Normalize the similarity matrix
normalized_similarity_matrix111 = min_max_normalize(FunSim)

# Create a DataFrame for the normalized matrix
FunSim_df = pd.DataFrame(normalized_similarity_matrix111, columns=Functional_df.columns, index=Functional_df.index)

import pandas as pd
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Assuming you have actual_df and predicted_df dataframes
file_path2 = '/content/drive/MyDrive/Integration/true positives.xlsx'
actual_df = pd.read_excel(file_path2).set_index('DOID')

# Combine the actual and predicted values for each pair of diseases
actual_values = actual_df.values.flatten()
predicted_values = FunSim_df.values.flatten()

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)


# Compute the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Functional Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

file_path2 = '/content/drive/MyDrive/Integration/true positives.xlsx'
actual_df = pd.read_excel(file_path2).set_index('DOID')

import pandas as pd
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Assuming you have actual_df and predicted_df dataframes for different methods

# Define methods and corresponding dataframes
methods = ['FunSim']  # Add more methods as needed
dataframes = [FunSim_df  ]  # Add corresponding dataframes

plt.figure(figsize=(10, 6))

for method, predicted_df in zip(methods, dataframes):
    # Combine the actual and predicted values for each pair of diseases
    actual_values = actual_df.values.flatten()
    predicted_values = predicted_df.values.flatten()

    # Compute the ROC curve
    fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

    # Compute the Area Under the Curve (AUC)
    roc_auc = auc(fpr, tpr)

    # Plot the ROC curve
    plt.plot(fpr, tpr, lw=2, label=f'{method} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Different Methods')
plt.legend(loc='lower right')
plt.show()

import pandas as pd
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Assuming you have actual_df and predicted_df dataframes for different methods

# Define methods and corresponding dataframes
methods = ['DiGeS-FN','Autoimmune','SemFunSim', 'FunSim']  # Add more methods as needed
dataframes = [ours, autoimmune_df, SEMFUNSIM_df, FunSim  ]  # Add corresponding dataframes

plt.figure(figsize=(10, 6))

for method, predicted_df in zip(methods, dataframes):
    # Combine the actual and predicted values for each pair of diseases
    actual_values = actual_df.values.flatten()
    predicted_values = predicted_df.values.flatten()

    # Compute the ROC curve
    fpr, tpr, thresholds = roc_curve(actual_values, predicted_values)

    # Compute the Area Under the Curve (AUC)
    roc_auc = auc(fpr, tpr)

    # Plot the ROC curve
    plt.plot(fpr, tpr, lw=2, label=f'{method} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Different Methods')
plt.legend(loc='lower right')
plt.show()

file_path3 = '/content/drive/MyDrive/Integration/semfunsim_matrix_final - Copy.xlsx'
S_df = pd.read_excel(file_path3).set_index('DOID')

S_N_df = min_max_normalize(S_df)
Sem_df = pd.DataFrame(S_N_df, columns=S_df.columns, index=S_df.index)

file_path1 = '/content/drive/MyDrive/Fun Sim/Functional_Similarity.xlsx'
F_df= pd.read_excel(file_path1).set_index('DOID')

F_N_df = min_max_normalize(F_df)
Fun_df = pd.DataFrame(F_N_df, columns=F_df.columns, index=F_df.index)

file_path5 = '/content/drive/MyDrive/Integration/CSM.xlsx'
N_df = pd.read_excel(file_path5).set_index('DOID')

N_N_df = min_max_normalize(N_df)
autoimmune_df = pd.DataFrame(N_N_df, columns=N_df.columns, index=N_df.index)

file_path3 = '/content/F.xlsx'
F= pd.read_excel(file_path3)

F_new = F.drop('Similarity', axis=1)

prefix_to_remove = 'DOID:'
F_new['Disease 1'] = F_new['Disease 1'].str.replace(prefix_to_remove, '')
F_new['Disease 2'] = F_new['Disease 2'].str.replace(prefix_to_remove, '')

print(F_new)

file_path3 = '/content/S (2).xlsx'
S= pd.read_excel(file_path3)

S_new = S.drop('Similarity', axis=1)

prefix_to_remove = 'DOID:'
S_new['Disease 1'] = S_new['Disease 1'].str.replace(prefix_to_remove, '')
S_new['Disease 2'] = S_new['Disease 2'].str.replace(prefix_to_remove, '')

print(S_new)

file_path3 = '/content/N (1).xlsx'
N= pd.read_excel(file_path3)

N_new = N.drop('Similarity', axis=1)

prefix_to_remove = 'DOID:'
N_new['Disease 1'] = N_new['Disease 1'].str.replace(prefix_to_remove, '')
N_new['Disease 2'] = N_new['Disease 2'].str.replace(prefix_to_remove, '')

print(N_new)

import pandas as pd

# Example DataFrames
df1 = pd.DataFrame({
    'Disease 1': [6132, 13809, 12365, 8469, 423],
    'Disease 2': [2841, 83, 8469, 12365, 633]
})

df2 = pd.DataFrame({
    'Disease 1': [83, 138091, 12336, 5844, 633],
    'Disease 2': [13809, 83, 5844, 12336, 423]
})

# Normalize the pairs in each DataFrame
df1_normalized = df1.apply(lambda row: tuple(sorted([row['Disease 1'], row['Disease 2']])), axis=1)
df2_normalized = df2.apply(lambda row: tuple(sorted([row['Disease 1'], row['Disease 2']])), axis=1)

# Find common pairs
common_pairs = set(df1_normalized) & set(df2_normalized)

# Convert the result back to a DataFrame
common_pairs_df = pd.DataFrame(list(common_pairs), columns=['Disease 1', 'Disease 2'])

# Print the resulting DataFrame
print(common_pairs_df)

print('Common pairs among F and S')

# Normalize the pairs in each DataFrame
df1_normalized = F_new.apply(lambda row: tuple(sorted([row['Disease 1'], row['Disease 2']])), axis=1)
df2_normalized = S_new.apply(lambda row: tuple(sorted([row['Disease 1'], row['Disease 2']])), axis=1)

# Find common pairs
common_pairs = set(df1_normalized) & set(df2_normalized)

# Convert the result back to a DataFrame
common_pairs_df = pd.DataFrame(list(common_pairs), columns=['Disease 1', 'Disease 2'])

# Print the resulting DataFrame
print(common_pairs_df)

print('Common pairs among F and N')

# Normalize the pairs in each DataFrame
df1_normalized = F_new.apply(lambda row: tuple(sorted([row['Disease 1'], row['Disease 2']])), axis=1)
df2_normalized = N_new.apply(lambda row: tuple(sorted([row['Disease 1'], row['Disease 2']])), axis=1)

# Find common pairs
common_pairs = set(df1_normalized) & set(df2_normalized)

# Convert the result back to a DataFrame
common_pairs_df = pd.DataFrame(list(common_pairs), columns=['Disease 1', 'Disease 2'])

# Print the resulting DataFrame
print(common_pairs_df)

print('Common pairs among S and N')

# Normalize the pairs in each DataFrame
df1_normalized = S_new.apply(lambda row: tuple(sorted([row['Disease 1'], row['Disease 2']])), axis=1)
df2_normalized = N_new.apply(lambda row: tuple(sorted([row['Disease 1'], row['Disease 2']])), axis=1)

# Find common pairs
common_pairs = set(df1_normalized) & set(df2_normalized)

# Convert the result back to a DataFrame
common_pairs_df = pd.DataFrame(list(common_pairs), columns=['Disease 1', 'Disease 2'])

# Print the resulting DataFrame
print(common_pairs_df)

print('Common pairs among F, S and N')

import pandas as pd

# Normalize the pairs in each DataFrame
df1_normalized = F_new.apply(lambda row: tuple(sorted([row['Disease 1'], row['Disease 2']])), axis=1)
df2_normalized = S_new.apply(lambda row: tuple(sorted([row['Disease 1'], row['Disease 2']])), axis=1)
df3_normalized = N_new.apply(lambda row: tuple(sorted([row['Disease 1'], row['Disease 2']])), axis=1)

# Find common pairs
common_pairs = set(df1_normalized) & set(df2_normalized) & set(df3_normalized)

# Convert the result back to a DataFrame
common_pairs_df = pd.DataFrame(list(common_pairs), columns=['Disease 1', 'Disease 2'])

# Print the resulting DataFrame
print(common_pairs_df)

pip install matplotlib seaborn

import numpy as np  # If using numpy arrays
import pandas as pd  # If using pandas DataFrame
import matplotlib.pyplot as plt
import seaborn as sns

print(ours_df)

# Convert DataFrame to numpy array (distance matrix)
data_matrix = ours_df.values

# Calculate distance matrix (if needed, depends on your data)
distance_matrix = 1 - data_matrix

# Perform hierarchical clustering
linkage_matrix = hierarchy.linkage(distance_matrix, method='complete')

# Plot dendrogram to visualize clustering
plt.figure(figsize=(10, 6))
dendrogram = hierarchy.dendrogram(linkage_matrix, labels=ours_df.index, orientation='right')
plt.title('Dendrogram of Hierarchical Clustering')
plt.xlabel('Distance')
plt.ylabel('Disease IDs')
plt.tight_layout()
plt.show()

# Plotting the heatmap
plt.figure(figsize=(47, 47))
sns.heatmap(data_matrix, annot=True, cmap='viridis', fmt='.3f', linewidths=.5, xticklabels=ours_df.index, yticklabels=ours_df.index)
plt.title('Heatmap of Data Matrix with Hierarchical Clustering')
plt.xlabel('Disease IDs')
plt.ylabel('Disease IDs')
plt.show()

